三、论文查重算法原理

1、什么是simhash

simhash是google于2007年发布的一篇论文《Detecting Near-duplicates for web crawling》中提出的算法，初衷是用于解决亿万级别的网页去重任务，simhash通常用于长文本，通过降维处理，将长文本压缩至几个关键词来代表一篇文章，然后再将这些关键词编码成一个固定长度的二进制字符串(一般为32位或是64位)，这样即用一个固定长度的编码来表示一整篇文章，我们想要对比多篇文章，只需要对比这些固定长度的编码就可以了。

2、simhash步骤

(1)、既然要用几个关键词来代替一篇文章，那么这几个关键词首先得有代表性，像类似“的”、“了”这种几乎每篇文章都会存在的词自然没有什么代表性，所以首先对文章分词后去停用词。

(2)、去停用词后计算每个词的tf-idf得分，关于tf-idf网上也有很多，在这里就不做过多介绍了，可以理解为是提取了文章中比较重要的词，并根据重要程度对每个词有个权重W，然后我们提取权重分值前N个关键词，比如对一篇文章我们选取前5个关键词和他们对应的tf-idf权重为：

(人工智能，1)、(大数据，2)、(科技，3)、(互联网，4)、(机器学习，5)

(3)、接下来需要对这些关键词进行编码，首先对5个词进行普通的hash之后得到一个N位(一般为32位或是64位)的二进制，为了方便书写，这里假设N为5，则：

人工智能：00101

大数据：11001

科技：00110

互联网：10101

机器学习：01011

(4)、针对每一个hash后的词，相应位置是1的，权重W取正，相应位置是0的，权重W取负，及变为：

人工智能：00101 ---> [-1,-1,1,-1,1]

大数据：11001 ---> [2,2,-2,-2,2]

科技：00110 ---> [-3,-3,3,3,-3]

互联网：10101 ---> [4,-4,4,-4,4]

机器学习：01011 ---> [-5,5,-5,5,5]

(5)、对上述变换后的列表进行列向累加得到：[-3, -1, 1, 1, 9]

(6)、对上述累加后的结果进行变换，对应位置为正数时取1，对应位置为负数时取0。

即：[-4, -1, 1, 1, 9] ---> 00111

这样就得到了一个文档的simhash值。即我们把一篇文章压缩成了一个固定长度的编码：00111